{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# From Features to Forecasts: Predicting Home Prices Using Computational Statistics\n",
        "Angeliki Andreadi, Laila Ibrahim, Salsabil Mtiraoui\n",
        "\n",
        "\n",
        "## 1. Library Imports"
      ],
      "metadata": {
        "id": "NfEFsIlbLGcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General use\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import io\n",
        "import sys\n",
        "\n",
        "# Preprossessing/Variables selection\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from itertools import combinations\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Time Series\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX"
      ],
      "metadata": {
        "id": "GHE5i5FQjW36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Loading"
      ],
      "metadata": {
        "id": "0fVtSgDe4Hwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 To execute on VSCode"
      ],
      "metadata": {
        "id": "wjCrV7Nq4WRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploaded csv => train.csv\n",
        "\n",
        "sys.path.append(\"house-prices-advanced-regression-techniques\")\n",
        "df = pd.read_csv('house-prices-advanced-regression-techniques/train.csv')"
      ],
      "metadata": {
        "id": "2Pz-d-NufMTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2  To execute on Google Colab"
      ],
      "metadata": {
        "id": "fCbQnJXZ4dj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploaded file => train.csv\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Automatically get the correct filename, no matter what it's called\n",
        "filename = next(iter(uploaded))\n",
        "print(f\"Using file: {filename}\")\n",
        "\n",
        "# Read the uploaded CSV file\n",
        "df = pd.read_csv(io.BytesIO(uploaded[filename]))"
      ],
      "metadata": {
        "id": "pgvFbLffbZx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Pre-prossessing"
      ],
      "metadata": {
        "id": "bV-MKStg5geN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the Id column since we don't need it\n",
        "df = df.drop('Id', axis=1)\n",
        "\n",
        "# We will replace NaN with \"none\",to make it easier to work with it\n",
        "df['Alley'] = df['Alley'].fillna('NA')\n",
        "\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "mmXD6N-rEGRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Separating each data type"
      ],
      "metadata": {
        "id": "2NmXAA5n5rl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute number of unique values for each column of df\n",
        "n_unique = df.nunique()\n",
        "# Determines if data is binary or categorical\n",
        "\n",
        "# Detrieves data type for each col in df\n",
        "dtypes = df.dtypes\n",
        "\n",
        "# Threshold after which the data is considered quantitative\n",
        "quant_threshold = 2\n",
        "\n",
        "# Variables where each sorted column's name will be stocked\n",
        "quant_var = []\n",
        "binary_var = []\n",
        "categorical_var = []\n",
        "\n",
        "\n",
        "for col in df.columns:\n",
        "  if dtypes[col] in ['int64', 'float64']:\n",
        "    if n_unique[col] == 2:\n",
        "      # If column's datatype is numbers and has only two possible values\n",
        "      binary_var.append(col)\n",
        "\n",
        "    elif n_unique[col]>quant_threshold:\n",
        "      # If column's datatype is numbers but has more than quant_threshold\n",
        "      quant_var.append(col)\n",
        "    else:\n",
        "      categorical_var.append(col)\n",
        "\n",
        "  elif dtypes[col] in ['object', 'category', 'bool']:\n",
        "        if n_unique[col] == 2:\n",
        "            if df[col].dtype == 'object':\n",
        "                # If the column is an object type but only has two values, convert it to binary\n",
        "                df[col] = df[col].map({df[col].unique()[0]: 0, df[col].unique()[1]: 1})\n",
        "            binary_var.append(col)\n",
        "        else:\n",
        "            categorical_var.append(col)\n",
        "\n",
        "# New data frames for each type\n",
        "\n",
        "df_quant = df[quant_var] # Quantitative variables\n",
        "df_binary = df[binary_var] # Binary variables\n",
        "df_categorical = df[categorical_var] # Categorical variables\n",
        "\n",
        "print(\"Quantitative variables:\", quant_var)\n",
        "print(\"Binary variables:\", binary_var)\n",
        "print(\"Categorical variables:\", categorical_var)\n",
        "\n",
        "# Checking that all our variables are here\n",
        "print(len(quant_var) + len(binary_var) + len(categorical_var))\n"
      ],
      "metadata": {
        "id": "3AOkdmQ8fvnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Dropping incomplete columns"
      ],
      "metadata": {
        "id": "9SRGkV1e6Bgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantitative\n",
        "\n",
        "# Total count for each variable\n",
        "total_counts_quant = df_quant.apply(lambda x: x.value_counts().sum())\n",
        "\n",
        "# Keep only the variables with a total count of 1460\n",
        "df_quant = df_quant.loc[:, total_counts_quant == 1460]\n",
        "df_quant"
      ],
      "metadata": {
        "id": "H1p7bTs3RxXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical\n",
        "\n",
        "# Total count for each variable\n",
        "total_counts = df_categorical.apply(lambda x: x.value_counts().sum())\n",
        "\n",
        "# Keep only the variables with a total count of 1460\n",
        "filtered_cat = df_categorical.loc[:, total_counts == 1460]\n",
        "\n",
        "# From now, we'll work with this instead of df_categorical\n",
        "filtered_cat"
      ],
      "metadata": {
        "id": "5M6aZHlZAOqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Feature Selection"
      ],
      "metadata": {
        "id": "4ftwV6x56M0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Quantitative variables"
      ],
      "metadata": {
        "id": "ZkIFGvlF6uFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def descriptive_stats(df_quand):\n",
        "  # Summary for each variable\n",
        "  stats_summary = df_quant.describe().T\n",
        "\n",
        "  for col in df_quant.columns:\n",
        "    mean = df_quant[col].mean()\n",
        "    sd = df_quant[col].std()\n",
        "    n = df_quant[col].count()\n",
        "    CI = stats.t.interval(0.95, n-1, loc = mean, scale= sd/np.sqrt(n))\n",
        "\n",
        "    stats_summary.loc[col, \"CI_95%_lower\"] = CI[0]\n",
        "    stats_summary.loc[col, \"CI_95%_upper\"] = CI[1]\n",
        "\n",
        "  return stats_summary\n",
        "\n",
        "\n",
        "def hypothesis_test(def_quant, price):\n",
        "  res = {}\n",
        "\n",
        "  for col in df_quant.columns:\n",
        "    if col == price:\n",
        "      continue\n",
        "\n",
        "\n",
        "    corr, p_value = pearsonr(df_quant[col], df_quant[price])\n",
        "    res[col] = {\"Correlation coeff\" : corr, \"P-value\": p_value}\n",
        "\n",
        "  return res\n",
        "\n",
        "stats_summary = descriptive_stats(df_quant)\n",
        "stats_summary\n",
        "\n",
        "hypothesis_test_results = hypothesis_test(df_quant, \"SalePrice\")\n",
        "hypothesis_test_results = pd.DataFrame(hypothesis_test_results).T  # Transpose to get variables as index\n",
        "hypothesis_test_results.index.name = \"Variable\"\n",
        "hypothesis_test_results.columns = [\"Correlation Coefficient\", \"P-value\"]\n",
        "hypothesis_test_results\n",
        "# Display the table"
      ],
      "metadata": {
        "id": "NNcXVDqmkaiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1.2 Quantitative variable ranking\n"
      ],
      "metadata": {
        "id": "ZmnmaNVq6_DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_features(df_quant, target_variable):\n",
        "    stats_summary = descriptive_stats(df_quant)\n",
        "    hypothesis_results = hypothesis_test(df_quant, target_variable)\n",
        "\n",
        "    # Convert hypothesis test results to Df for easier manipulation\n",
        "    hypothesis_df = pd.DataFrame(hypothesis_results).T\n",
        "\n",
        "    # Combine both descriptive stats and hypothesis test results into one DataFrame\n",
        "    combined_df = stats_summary.join(hypothesis_df)\n",
        "    # We will rank by correlation, p-value, and the variance\n",
        "\n",
        "    # Rank by correlation coefficient (absolute value to consider positive and negative correlations)\n",
        "    combined_df['abs_corr'] = combined_df['Correlation coeff'].abs()\n",
        "\n",
        "    # Add a score based on the variance (higher variance = more spread = more informative)\n",
        "    combined_df['variance'] = combined_df['std'].rank(ascending=False, method='min')\n",
        "\n",
        "    # Add a score based on p-value (lower p-value = higher significance)\n",
        "    combined_df['p_value_rank'] = combined_df['P-value'].rank(ascending=True, method='min')\n",
        "\n",
        "    combined_df['final_rank'] = combined_df['p_value_rank']\n",
        "\n",
        "    # Final rank score (each factor is kind of arbirtarily weighted, the more * the more important it is considered)\n",
        "    #combined_df['final_rank'] = combined_df['abs_corr'] * 10 + combined_df['p_value_rank'] * 2 + combined_df['variance'] * 5\n",
        "\n",
        "\n",
        "    combined_df_sorted = combined_df.sort_values(by='final_rank', ascending=True)\n",
        "\n",
        "    return combined_df_sorted[['Correlation coeff', 'P-value', 'CI_95%_lower', 'CI_95%_upper', 'variance', 'final_rank']]\n",
        "\n",
        "# Example usage (assuming df_quant is your quantitative DataFrame and \"SalePrice\" is the target variable)\n",
        "ranked_features = rank_features(df_quant, \"SalePrice\")\n",
        "print(ranked_features)"
      ],
      "metadata": {
        "id": "isMuJ9PQzW8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Binary variables"
      ],
      "metadata": {
        "id": "5QuOA1cPDbQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive stats for binary variables\n",
        "desc_stats_binary = df_binary.describe().T\n",
        "print(desc_stats_binary)\n",
        "\n",
        "# Correlation with SalePrice for binary variables\n",
        "df_binary.corrwith(df[\"SalePrice\"]).sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "fRz8I-ZunsDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.1 $2^k$ factorial design"
      ],
      "metadata": {
        "id": "NAN6louL99dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factors = [\"Street\", \"Utilities\", \"CentralAir\"]\n",
        "df_factors = df[factors + [\"SalePrice\"]].dropna()\n",
        "\n",
        "# Build formula with all main effects + 2-way + 3-way interactions\n",
        "main_effects = \"+\".join(factors)\n",
        "interactions = \"+\".join([\":\".join(combo) for i in range(2, 4) for combo in combinations(factors, i)])\n",
        "formula = f\"SalePrice ~ {main_effects} + {interactions}\"\n",
        "\n",
        "# Fit linear model\n",
        "model = smf.ols(formula=formula, data=df_factors).fit()\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "48XtznWOGjtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Categorical variables"
      ],
      "metadata": {
        "id": "CLRorHJMEfJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.Feature selection"
      ],
      "metadata": {
        "id": "N2H-TIY5Atc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics for categorical variables\n",
        "df_categorical.describe()\n",
        "\n",
        "# For the rest of the project, we will work with 'filtered_cat', which represents our filtered categorical variables"
      ],
      "metadata": {
        "id": "7Xtsu-zn442e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_categorical_vars(df, filtered_cat, prices):\n",
        "  results = []\n",
        "  # Loop through each cat var\n",
        "  for col in filtered_cat:\n",
        "    try:\n",
        "      # Build list of arrays, each array is Sale Price for on category of the cat variable.\n",
        "      # Ex. makes array for prices roofStyle == \"Flat\"\n",
        "      groups = [df[prices][filtered_cat[col]== cat].dropna() for cat in filtered_cat[col].unique()]\n",
        "\n",
        "      if len(groups) > 1:\n",
        "        # One way anova since we have one quant and several categorical\n",
        "          f_stat, p_val = f_oneway(*groups)\n",
        "          # * passes each item as a separate argument\n",
        "\n",
        "          # Compute diff between highest and lowest mean across the categories, to sense how big the effect is\n",
        "          mean_range = max([g.mean() for g in groups]) - min([g.mean() for g in groups])\n",
        "          results.append({\n",
        "              \"Variable\": col,\n",
        "              \"F-value\": f_stat,\n",
        "              \"P_value\": p_val,\n",
        "              \"Mean_range\": mean_range\n",
        "          })\n",
        "    except Exception as e:\n",
        "        print(f\"Could not process {col}: {e}\")\n",
        "        continue\n",
        "  df_results = pd.DataFrame(results)\n",
        "  df_results[\"p_rank\"] = df_results[\"P_value\"].rank(ascending=True)\n",
        "  df_results[\"range_rank\"] = df_results[\"Mean_range\"].rank(ascending=False)\n",
        "\n",
        "    # Arbitrary scoring: lower p, higher range = better\n",
        "  df_results[\"final_score\"] = df_results[\"p_rank\"] * 2 + df_results[\"range_rank\"]\n",
        "  # lower score = better variable, bcs lower p rank = + significant p-value and high range rank = large spread between group means\n",
        "  df_results = df_results.sort_values(by=\"final_score\")#.rank(ascending=False)\n",
        "\n",
        "  return df_results\n",
        "\n",
        "categorical_rankings = rank_categorical_vars(df, filtered_cat, \"SalePrice\")\n",
        "print(categorical_rankings)\n",
        "\n",
        "# Sort by p-value for cleaner visualization\n",
        "df_sorted = categorical_rankings.sort_values(\"P_value\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_sorted, x=\"Variable\", y=\"P_value\", palette=\"viridis\")\n",
        "\n",
        "plt.axhline(0.05, color='red', linestyle='--', label='Significance threshold (0.05)')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel(\"P-value\")\n",
        "plt.title(\"P-values from ANOVA for Categorical Variables\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#\n",
        "df_sorted = categorical_rankings.sort_values(\"Mean_range\", ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_sorted, x=\"Variable\", y=\"Mean_range\", palette=\"coolwarm\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel(\"Range of Means\")\n",
        "plt.title(\"Range of SalePrice Means Across Categories\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Sort so that top-ranked variables are on top of the plot\n",
        "df_sorted = categorical_rankings.sort_values(\"final_score\", ascending=True)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_sorted, x=\"final_score\", y=\"Variable\", palette=\"viridis\")\n",
        "plt.xlabel(\"Final Score\")\n",
        "plt.ylabel(\"Categorical Variable\")\n",
        "plt.title(\"Ranking of Categorical Variables Based on Final Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Low P-values means the categorical value likely affects the SalesPrice, it is not due to chance\n"
      ],
      "metadata": {
        "id": "1OnKV7PIzY_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "top_cat_vars = categorical_rankings.sort_values(\"final_score\").head(10)[\"Variable\"].tolist()\n",
        "\n",
        "#  One-hot encoding\n",
        "df_encoded_cat = pd.get_dummies(filtered_cat[top_cat_vars], drop_first=True).astype('float64')\n",
        "\n",
        "#  Ajouter la constante (intercept)\n",
        "X_cat = sm.add_constant(df_encoded_cat)\n",
        "\n",
        "#  Préparer la variable cible\n",
        "y = df[\"SalePrice\"].astype('float64')\n",
        "\n",
        "#  Nettoyage — garder uniquement les lignes sans NaN\n",
        "X_cat, y = X_cat.align(y, join='inner', axis=0)\n",
        "\n",
        "# Régression linéaire\n",
        "model_cat = sm.OLS(y, X_cat).fit()\n",
        "\n",
        "print(model_cat.summary())"
      ],
      "metadata": {
        "id": "Eof8iZLjBg47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort coefficients outside the constant and keep only the most significant ones\n",
        "coeffs = model_cat.params.drop(\"const\").sort_values(key=abs, ascending=False).head(20)  # top 20 coeffs en valeur absolue\n",
        "\n",
        "# We shorten the too long names\n",
        "labels = [label if len(label) <= 25 else label[:22] + \"...\" for label in coeffs.index]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.barplot(x=coeffs.values, y=labels, palette=\"coolwarm\")\n",
        "plt.axvline(x=0, color='black', linestyle='--')\n",
        "plt.title(\"Top 20 modalités catégorielles impactant le prix de vente\", fontsize=14)\n",
        "plt.xlabel(\"Coefficient estimé (variation du prix)\", fontsize=12)\n",
        "plt.ylabel(\"Modalités\", fontsize=12)\n",
        "plt.grid(axis='x', linestyle=':', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zvgM7HUnBvw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of quantitative variables by correlation and p-value\n",
        "selected_quant_vars = [\n",
        "    \"OverallQual\",\n",
        "    \"GrLivArea\",\n",
        "    \"GarageCars\",\n",
        "    \"GarageArea\",\n",
        "    \"TotalBsmtSF\",\n",
        "    \"1stFlrSF\",\n",
        "    \"FullBath\",\n",
        "    \"TotRmsAbvGrd\",\n",
        "    \"YearBuilt\",\n",
        "    \"YearRemodAdd\"\n",
        "]\n",
        "\n",
        "# Sub-dataframe with these variables only\n",
        "df_selected_quant = df[selected_quant_vars].copy()\n",
        "\n",
        "\n",
        "# Selection of the top 10 most relevant categorical variables\n",
        "top_cat_vars = categorical_rankings.sort_values(\"final_score\").head(10)[\"Variable\"].tolist()\n",
        "df_encoded_cat = pd.get_dummies(filtered_cat[top_cat_vars], drop_first=True)\n",
        "\n",
        "# Merge with selected quantitative variables\n",
        "X_combined = pd.concat([df_selected_quant, df_encoded_cat], axis=1)\n",
        "\n",
        "# Explicit conversion to float and deletion of non-numeric columns\n",
        "X_combined_clean = X_combined.copy()\n",
        "for col in X_combined.columns:\n",
        "    try:\n",
        "        X_combined_clean[col] = X_combined[col].astype(float)\n",
        "    except:\n",
        "        X_combined_clean.drop(columns=col, inplace=True)\n",
        "\n",
        "# Add constant and align with y\n",
        "X_combined_clean = sm.add_constant(X_combined_clean, has_constant='add')\n",
        "y_clean = df[\"SalePrice\"]\n",
        "\n",
        "# Cleaning\n",
        "valid_index = X_combined_clean.dropna().index.intersection(y_clean.dropna().index)\n",
        "X_final = X_combined_clean.loc[valid_index]\n",
        "y_final = y_clean.loc[valid_index]\n",
        "\n",
        "# Linear Regression\n",
        "model = sm.OLS(y_final, X_final).fit()\n",
        "print(model.summary())\n",
        "\n",
        "# Residual analysis\n",
        "residuals = model.resid\n",
        "fitted = model.fittedvalues\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=fitted, y=residuals)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title(\"Résidus vs valeurs ajustées\")\n",
        "plt.xlabel(\"Valeurs ajustées\")\n",
        "plt.ylabel(\"Résidus\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(residuals, kde=True, color='skyblue')\n",
        "plt.title(\"Distribution des résidus\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "sm.qqplot(residuals, line='s')\n",
        "plt.title(\"Q-Q Plot des résidus\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LSBMAM-AB5yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Time Series Analysis"
      ],
      "metadata": {
        "id": "rUeSPV_ibVIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregatting month and year sold dates\n",
        "df_quant['SaleDate'] = pd.to_datetime(dict(year=df_quant['YrSold'], month=df_quant['MoSold'], day=1))\n",
        "\n",
        "monthly_avg_prices = df_quant.groupby('SaleDate')['SalePrice'].mean().reset_index()\n",
        "\n",
        "\n",
        "plt.figure(figsize = (8, 5))\n",
        "plt.plot(monthly_avg_prices['SaleDate'], monthly_avg_prices['SalePrice'], '.', alpha = 0.5, linestyle = \"-\", color=\"blue\")\n",
        "plt.xlabel(\"Sale Date\")\n",
        "plt.ylabel(\"Sale Price\")\n",
        "plt.title(\"Average house Sale Prices Over Time\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "result = seasonal_decompose(monthly_avg_prices['SalePrice'], model='additive', period=12)\n",
        "result.plot()"
      ],
      "metadata": {
        "id": "inempo6w5SVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Smoothing (Moving Average)\n",
        "# Removes short term fluctuations and highlights long term trends\n",
        "\n",
        "monthly_avg_prices['MA6'] = monthly_avg_prices['SalePrice'].rolling(window=6).mean()\n",
        "monthly_avg_prices['MA6'].plot() # bleu\n",
        "\n",
        "monthly_avg_prices['MA12'] = monthly_avg_prices['SalePrice'].rolling(window=12).mean()\n",
        "monthly_avg_prices['MA12'].plot() # orange\n",
        "plt.title(\"Smoothed House Prices Over Time\")\n",
        "plt.legend()\n",
        "\n",
        "# Both show the average price is slowly going down\n"
      ],
      "metadata": {
        "id": "xL5AfAoPQw7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stationarity test, ADF\n",
        "result2 = adfuller(monthly_avg_prices['SalePrice'].dropna())\n",
        "\n",
        "print(f'ADF Statistic: {result2[0]}, p-value: {result2[1]}')\n",
        "\n",
        "# if p-value is smaller than 0.05, can model directly"
      ],
      "metadata": {
        "id": "21j3x-ntRwEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acf(monthly_avg_prices['SalePrice'].dropna(), lags=20) # lag 20 = arbitrary choice,\n",
        "plt.grid(True)\n",
        "# If too small we miss long term patterns, too big too much noise, we compare with almost 2 years before\n",
        "\n",
        "\n",
        "plot_pacf(monthly_avg_prices['SalePrice'].dropna(), lags=20)\n",
        "plt.grid(True)\n",
        "\n",
        "# lag 4 is significant"
      ],
      "metadata": {
        "id": "diwGzu6VXvHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting AR, MA, ARMA models\n",
        "# AR(4)\n",
        "x = monthly_avg_prices['SalePrice'].dropna()\n",
        "\n",
        "ar_model = ARIMA(x, order=(4, 0, 0)).fit()\n",
        "\n",
        "#MA(4)\n",
        "ma_model = ARIMA(x, order=(0,0,4)).fit()\n",
        "\n",
        "#ARMA(4,4)\n",
        "arma_model = ARIMA(x, order=(4,0,0)).fit()\n",
        "\n",
        "print(\"AIC & BIC Scores\")\n",
        "print(\"AR(4):   AIC =\", ar_model.aic, \" | BIC =\", ar_model.bic)\n",
        "print(\"MA(4):   AIC =\", ma_model.aic, \" | BIC =\", ma_model.bic)\n",
        "print(\"ARMA(4,4): AIC =\", arma_model.aic, \" | BIC =\", arma_model.bic)"
      ],
      "metadata": {
        "id": "1CpSwUyhHdZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit ARIMA(4,0,0)\n",
        "model_ar = ARIMA(x, order=(4, 0, 0)).fit()\n",
        "\n",
        "# Fit ARIMA(0,0,4)\n",
        "model_ma = ARIMA(x, order=(0, 0, 4)).fit()\n",
        "\n",
        "# Fit SARIMAX(1,0,1)(1,0,0,12)\n",
        "model_sarimax = SARIMAX(x, order=(1, 0, 1), seasonal_order=(1, 0, 0, 12)).fit()\n",
        "sarima_pred = model_sarimax.predict(start=x.index[0], end=x.index[-1])\n",
        "\n",
        "# Subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Model Fits Comparison')\n",
        "\n",
        "# ARIMA(4,0,0)\n",
        "axs[0, 0].plot(x, label='Observed')\n",
        "axs[0, 0].plot(model_ar.fittedvalues, label='Fitted', color='orange')\n",
        "axs[0, 0].set_title('ARIMA(4,0,0)')\n",
        "axs[0, 0].legend()\n",
        "\n",
        "# ARIMA(0,0,4)\n",
        "axs[0, 1].plot(x, label='Observed')\n",
        "axs[0, 1].plot(model_ma.fittedvalues, label='Fitted', color='orange')\n",
        "axs[0, 1].set_title('ARIMA(0,0,4)')\n",
        "axs[0, 1].legend()\n",
        "\n",
        "# SARIMAX(1,0,1)(1,0,0,12)\n",
        "axs[1, 0].plot(x, label='Observed')\n",
        "axs[1, 0].plot(sarima_pred, label='Fitted', color='orange')\n",
        "axs[1, 0].set_title('SARIMAX(1,0,1)(1,0,0,12)')\n",
        "axs[1, 0].legend()\n",
        "\n",
        "# Empty subplot\n",
        "axs[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Print AIC and BIC for comparison\n",
        "print(\"Model Comparison:\")\n",
        "print(f\"ARIMA(4,0,0)    → AIC: {model_ar.aic:.2f}, BIC: {model_ar.bic:.2f}\")\n",
        "print(f\"ARIMA(0,0,4)    → AIC: {model_ma.aic:.2f}, BIC: {model_ma.bic:.2f}\")\n",
        "print(f\"SARIMAX         → AIC: {model_sarimax.aic:.2f}, BIC: {model_sarimax.bic:.2f}\")\n"
      ],
      "metadata": {
        "id": "S3auzA0hyIid"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}